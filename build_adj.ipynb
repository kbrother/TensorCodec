{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14251fd3-496b-4328-9828-2d2bfe31c9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                | 0/21764 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 59%|██████████████████████████████████████████████████████████████████████████████████████▊                                                             | 12769/21764 [11:58<03:34, 41.89it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_name, target_dim = \"brainq_mat\", 1\n",
    "data_file = \"../data/\" + data_name + \".npy\"\n",
    "_mat = np.load(data_file)\n",
    "batch_size = 2**21\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "num_node = _mat.shape[target_dim]\n",
    "seg_len = _mat.shape[1-target_dim]\n",
    "adj_mat = np.zeros((num_node, num_node))\n",
    "for i in tqdm(range(num_node)):\n",
    "    if target_dim == 0: slice1 = _mat[i,:]\n",
    "    else: slice1 = _mat[:,i]\n",
    "    slice1 = np.tile(slice1, num_node-i-1) \n",
    "    \n",
    "    if target_dim == 0: slice2 = _mat[i+1:,:].flatten()\n",
    "    else: slice2 = np.transpose(_mat[:,i+1:]).flatten()\n",
    "    slice1 = torch.tensor(slice1, device=device)\n",
    "    slice2 = torch.tensor(slice2, device=device)\n",
    "    \n",
    "    curr_len = seg_len*(num_node-i-1)\n",
    "    #print(num_node)\n",
    "    #print(_mat[:,i+1:].shape)\n",
    "    #print(f'curr len:{curr_len}, slice2:{slice2.shape}')\n",
    "    curr_adj_row = torch.zeros(num_node-i-1, device=device, dtype = torch.float64)    \n",
    "    for j in range(0, curr_len, batch_size):\n",
    "        if curr_len - j < batch_size: curr_batch_size = curr_len - j\n",
    "        else: curr_batch_size = batch_size           \n",
    "        curr_idx = torch.arange(j, j+curr_batch_size, device=device) // seg_len\n",
    "        curr_val = torch.square(slice1[j:j+curr_batch_size] - slice2[j:j+curr_batch_size])        \n",
    "        \n",
    "        #print(f'row:{curr_adj_row.shape}, idx:{curr_idx.shape}, val:{curr_val.shape}')\n",
    "        curr_adj_row.scatter_(0, curr_idx, curr_val, reduce='add')\n",
    "    \n",
    "    curr_adj_row = torch.sqrt(curr_adj_row)\n",
    "    curr_adj_row = curr_adj_row.clone().cpu().numpy()\n",
    "    adj_mat[i, i+1:] = curr_adj_row\n",
    "    adj_mat[i+1:, i] = curr_adj_row\n",
    "\n",
    "out_file = data_name + \"_adj\" + str(target_dim) + \".npy\"\n",
    "np.save(out_file, adj_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
